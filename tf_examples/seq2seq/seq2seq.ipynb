{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning seq2seq\n",
    "\n",
    "I'm trying to learn how seq2seq works. I understand it in principle but need to figure out how to actually implement it in TensorFlow. At this point, the seq2seq library is a little too dense for me to follow line by line, so I'll be following a smaller implementation from [Illia Polosukhin](https://medium.com/@ilblackdragon/tensorflow-sequence-to-sequence-3d9d2e238084)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some universals. Indices for start, end, and unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_TOKEN = 0\n",
    "END_TOKEN = 1\n",
    "UNK_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the seq2seq model. I'll try annotating with comments as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(mode, features, labels, params):\n",
    "    vocab_size = params['vocab_size'] # the number of permitted vocabulary words\n",
    "    embed_dim = params['embed_dim'] # the embedding dimension of the words\n",
    "    num_units = params['num_units'] # the embedding dimension of the GRU cell\n",
    "    input_max_length = params['input_max_length'] # the maximum accepted length for inputs\n",
    "    output_max_length = params['output_max_length'] # the maximum accepted length for outputs\n",
    "    \n",
    "    inp = features['input'] # input sequences from passed parameter\n",
    "    output = features['output'] # output sequences? how is it different from labels?\n",
    "    batch_size = tf.shape(inp)[0] # infer the batch size from the first dimension of the inputs\n",
    "    start_tokens = tf.zeros([batch_size], dtype=tf.int64) # make a vector for start\n",
    "    # tf.expand_dims makes start_tokens go from dim batch_size to dim batch_size x 1\n",
    "    # then we concat start_tokens to the front of output; axis 1 says concat by columns (hstack)\n",
    "    train_output = tf.concat([tf.expand_dims(start_tokens, 1), output], 1)\n",
    "    # tf.not_equal produces a tensor of same dim as train_input, True if != 1 (end_token), False otherwise\n",
    "    # tf.to_int32 changes bool to 1s and 0s\n",
    "    # tf.reduce_sum provides lengths\n",
    "    input_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(inp, 1)), 1)\n",
    "    output_lengths = tf.reduce_sum(tf.to_int32(tf.not_equal(train_output, 1)), 1)\n",
    "    # takes the indices and converts each sentence to series of embeddings\n",
    "    # scope indicates the variable scope of the op\n",
    "    input_embed = layers.embed_sequence(inp, \n",
    "                                       vocab_size=vocab_size,\n",
    "                                       embed_dim=embed_dim,\n",
    "                                       scope='embed')\n",
    "    output_embed = layers.embed_sequence(train_output, \n",
    "                                       vocab_size=vocab_size,\n",
    "                                       embed_dim=embed_dim,\n",
    "                                       scope='embed')\n",
    "    # variable_scope is a context manager for the creation of variables\n",
    "    # 'embed' is the name of the scope\n",
    "    # reuse=True, means that the embeddings can be shared, reused\n",
    "    # makes a variable called 'embeddings.' Not initialized.\n",
    "    with tf.variable_scope('embed', reuse=True):\n",
    "        embeddings = tf.get_variable('embeddings')\n",
    "        \n",
    "    # the cell of the encoder/decoder is GRU\n",
    "    cell = tf.contrib.rnn.GRUCell(num_units=num_units)\n",
    "    # this is the RNN, which provides outputs and the final state\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(cell, input_embed, dtype=tf.float32)\n",
    "    \n",
    "    # Still not sure what TrainingHelper does. It has something to do with decoding for producing outputs.\n",
    "    train_helper = tf.contrib.seq2seq.TrainingHelper(output_embed, output_lengths)\n",
    "    # This is the inference helper when there are no labels?\n",
    "    pred_helper = tf.conrib.seq2seqGreedyEmbeddingHelper(embeddings, start_tokens=tf.to_int32(start_tokens), end_token=1)\n",
    "    \n",
    "    # function for actual decoding, during training or inference    \n",
    "    def decode(helper, scope, reuse=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            # using attention\n",
    "            # need num_units in GRU state\n",
    "            # the outputs of the encoder are the input to attention\n",
    "            # also pass encoder input lengths\n",
    "            attention_mechanism = tf.contrib.seq2seqBahdanauAttention(num_units=num_units, memory=encoder_outputs, memory_sequence_length=input_lenghts)\n",
    "            # "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
