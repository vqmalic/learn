{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table Learning for FrozenLake\n",
    "\n",
    "This is an implementation of Q-Table learning for the reinforcement learning environment FrozenLake. This follows the tutorial from [awjuliani](https://gist.github.com/awjuliani/9024166ca08c489a60994e529484f7fe#file-q-table-learning-clean-ipynb).  \n",
    "\n",
    "## The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env` is a $4 \\times 4$ grid, with four types of tiles: S (starting), F (frozen), H (hole), and G (goal). The agent starts on the S tile and can choose one of four directions to move - 0 (left), 1 (down), 2 (right), and 3 (up). The agent must make it to the G tile while avoiding the H tile. Here's the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A run ends when the agent falls into an H tile (reward: 0) or the agent finds the goal (reward: 1). \n",
    "\n",
    "The catch: the frozen lake is slippery. When the agent chooses to go in a specific direction, there's only a 1/3 chance the agent will actually go in that direction. There's a 2/3rd chance that the agent will go perpendicular to the desired direction (1/3rd to the left, with respect to the desired direction, and 1/3rd to the right). Because of this schotastic element the environment cannot be perfectly solved. \n",
    "\n",
    "In fact, when you look at the finished Q-table as a sanity check, you'll find that the best strategy to take when located next to an H tile is to move *directly away from it,* even if that doesn't get you closer to the goal. This is the one way the agent can guarantee that it *won't* fall into the hole (except when you are adjacent to two holes, which occurs once, at row 2, column 3). I had to check the source code to verify the slippery behaviour; before I did that I was confused by the strategies learned by the Q-Table. \n",
    "\n",
    "The locations are index from 0 to 15, going along rows, then by columns.\n",
    "\n",
    "The Q-Table contains Q-values for state-action pairs. The Q-value for a state-action pair is the *estimated, expected value* of the reward for taking action $a$ when currently in state $s$. It takes the following form:\n",
    "\n",
    "$Q(s, a) = r + \\gamma(\\text{max}(Q(s', a'))$\n",
    "\n",
    "It's equal to the reward received haven entered state $s$ as well as the maximum discounted Q-value for the possible actions taken from this state. Each Q-value is updated according to Bellman's equation:\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha (r + \\gamma \\text{max}(Q(s', a')) - Q(s, a))$\n",
    "\n",
    "Where $\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 5000\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode: 0.5822\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table values\n",
      "[[0.163 0.008 0.006 0.005]\n",
      " [0.    0.001 0.    0.146]\n",
      " [0.001 0.009 0.003 0.097]\n",
      " [0.    0.001 0.002 0.097]\n",
      " [0.34  0.005 0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.025 0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.005 0.    0.005 0.36 ]\n",
      " [0.    0.654 0.001 0.001]\n",
      " [0.164 0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.001 0.892 0.003]\n",
      " [0.    0.    0.995 0.002]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Table values\")\n",
    "print(np.round(Q, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:'<', 1:'▼', 2:'>', 3:'▲'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '▲' '▲' '▲']\n",
      " ['<' 'O' '>' 'O']\n",
      " ['▲' '▼' '<' 'O']\n",
      " ['O' '>' '>' '!']]\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for state, actions in enumerate(np.round(Q, 3)):\n",
    "    if np.sum(actions) == 0:\n",
    "        out.append('O')\n",
    "    else:\n",
    "        out.append(dic[np.argmax(actions)])\n",
    "out[-1] = '!'\n",
    "out = np.array(out)\n",
    "out = out.reshape(4, 4)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learner has learned to be \"afraid\" of holes, whenever it is adjacent to one the optimal behaviour is to move directly away from it. There's trouble at row 2, column 3 because moving left or right will put the agent in a hole. Interestingly, in this situation, the agent usually learns the optimal action is to move towards one of the holes. This is, in fact, the best strategy, because if the agent moves up or down directly toward safe ground, it actually has a 2/3rd chance of slipping sideways into a hole, whereas if it goes directly toward either of the holes it has a 2/3rd chance of slipping into safety. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
