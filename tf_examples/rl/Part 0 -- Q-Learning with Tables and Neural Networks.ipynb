{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table Learning for FrozenLake\n",
    "\n",
    "This is an implementation of Q-Table learning for the reinforcement learning environment FrozenLake. This follows the tutorial from [awjuliani](https://gist.github.com/awjuliani/9024166ca08c489a60994e529484f7fe#file-q-table-learning-clean-ipynb).  \n",
    "\n",
    "## The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`env` is a $4 \\times 4$ grid, with four types of tiles: S (starting), F (frozen), H (hole), and G (goal). The agent starts on the S tile and can choose one of four directions to move - 0 (left), 1 (down), 2 (right), and 3 (up). The agent must make it to the G tile while avoiding the H tile. Here's the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A run ends when the agent falls into an H tile (reward: 0) or the agent finds the goal (reward: 1). \n",
    "\n",
    "The catch: the frozen lake is slippery. When the agent chooses to go in a specific direction, there's only a 1/3 chance the agent will actually go in that direction. There's a 2/3rd chance that the agent will go perpendicular to the desired direction (1/3rd to the left, with respect to the desired direction, and 1/3rd to the right). Because of this schotastic element the environment cannot be perfectly solved. \n",
    "\n",
    "In fact, when you look at the finished Q-table as a sanity check, you'll find that the best strategy to take when located next to an H tile is to move *directly away from it,* even if that doesn't get you closer to the goal. This is the one way the agent can guarantee that it *won't* fall into the hole (except when you are adjacent to two holes, which occurs once, at row 2, column 3). I had to check the source code to verify the slippery behaviour; before I did that I was confused by the strategies learned by the Q-Table. \n",
    "\n",
    "The locations are index from 0 to 15, going along rows, then by columns.\n",
    "\n",
    "The Q-Table contains Q-values for state-action pairs. The Q-value for a state-action pair is the *estimated, expected value* of the reward for taking action $a$ when currently in state $s$. It takes the following form:\n",
    "\n",
    "$Q(s, a) = r + \\gamma(\\text{max}(Q(s', a'))$\n",
    "\n",
    "It's equal to the reward received haven entered state $s$ as well as the maximum discounted Q-value for the possible actions taken from this state. Each Q-value is updated according to Bellman's equation:\n",
    "\n",
    "$Q(s, a) \\leftarrow Q(s, a) + \\alpha (r + \\gamma \\text{max}(Q(s', a')) - Q(s, a))$\n",
    "\n",
    "Where $\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 5000\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    while j < 99:\n",
    "        j += 1\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1, env.action_space.n) * (1./(i+1)))\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode: 0.5496\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table values\n",
      "[[0.217 0.007 0.009 0.   ]\n",
      " [0.002 0.004 0.002 0.151]\n",
      " [0.002 0.002 0.006 0.242]\n",
      " [0.    0.001 0.    0.068]\n",
      " [0.346 0.    0.001 0.002]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.017 0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.001 0.    0.003 0.225]\n",
      " [0.    0.235 0.002 0.002]\n",
      " [0.116 0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.    0.    0.202 0.001]\n",
      " [0.    0.    0.    0.577]\n",
      " [0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q Table values\")\n",
    "print(np.round(Q, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0:'<', 1:'▼', 2:'>', 3:'▲'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<' '▲' '▲' '▲']\n",
      " ['<' 'O' '<' 'O']\n",
      " ['▲' '▼' '<' 'O']\n",
      " ['O' '>' '▲' '!']]\n"
     ]
    }
   ],
   "source": [
    "out = []\n",
    "for state, actions in enumerate(np.round(Q, 3)):\n",
    "    if np.sum(actions) == 0:\n",
    "        out.append('O')\n",
    "    else:\n",
    "        out.append(dic[np.argmax(actions)])\n",
    "out[-1] = '!'\n",
    "out = np.array(out)\n",
    "out = out.reshape(4, 4)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the learner has learned to be \"afraid\" of holes, whenever it is adjacent to one the optimal behaviour is to move directly away from it. There's trouble at row 2, column 3 because moving left or right will put the agent in a hole. Interestingly, in this situation, the agent usually learns the optimal action is to move towards one of the holes. This is, in fact, the best strategy, because if the agent moves up or down directly toward safe ground, it actually has a 2/3rd chance of slipping sideways into a hole, whereas if it goes directly toward either of the holes it has a 2/3rd chance of slipping into safety. \n",
    "\n",
    "# Q-Learning with Tensorflow and Neural Networks\n",
    "\n",
    "Alright, let's do this with a neural network that determines the best action to take. \n",
    "\n",
    "The input to the neural network is the state - the location of the agent - which is represented in as a one-hot vector. The output is a vector of 4 Q-values associated with each of the possible actions to take. The loss function is a sum-of-squares loss, derived from the difference between the network's predicted Q-values and the target Q-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tf.placeholder(shape=[1, 16], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1, W)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ-Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episodes completed.\n",
      "100 episodes completed.\n",
      "200 episodes completed.\n",
      "300 episodes completed.\n",
      "400 episodes completed.\n",
      "500 episodes completed.\n",
      "600 episodes completed.\n",
      "700 episodes completed.\n",
      "800 episodes completed.\n",
      "900 episodes completed.\n",
      "1000 episodes completed.\n",
      "1100 episodes completed.\n",
      "1200 episodes completed.\n",
      "1300 episodes completed.\n",
      "1400 episodes completed.\n",
      "1500 episodes completed.\n",
      "1600 episodes completed.\n",
      "1700 episodes completed.\n",
      "1800 episodes completed.\n",
      "1900 episodes completed.\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "y = 0.99\n",
    "e = 0.1 # epsilon, chance of taking a random action\n",
    "num_episodes = 2000\n",
    "\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j += 1\n",
    "            # np.identity makes an identity matrix; 1s along diagonal\n",
    "            # get one-hot encoding by selecting the sth row\n",
    "            # use list slicing to get a 1 x 16 encoding\n",
    "            a, allQ = sess.run([predict, Qout], feed_dict = {inputs1: np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            s1, r, d, _ = env.step(a[0])\n",
    "            # Q value for this state\n",
    "            Q1 = sess.run(Qout, feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "            # get the max\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0, a[0]] = r + y * maxQ1\n",
    "            _, W1 = sess.run([updateModel, W], feed_dict={inputs1:np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                # decrease epsilon as training occurs\n",
    "                e = 1./((i/50) + 10)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        if i % 100 == 0:\n",
    "            print(\"{} episodes completed.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode: 0.439\n"
     ]
    }
   ],
   "source": [
    "print(\"Average reward per episode: {}\".format(sum(rList)/num_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
